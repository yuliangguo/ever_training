{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amai/gaussian-splatting-merge\n",
      "['-std=c++17']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CC\"] = \"/usr/bin/gcc-11\"\n",
    "os.environ[\"CXX\"] = \"/usr/bin/g++-11\"\n",
    "import struct\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(str(Path(os.path.abspath('')).parent))\n",
    "print(str(Path(os.path.abspath('')).parent))\n",
    "import torch\n",
    "from gaussian_renderer import GaussianModel, splinerender, render\n",
    "from scene import Scene\n",
    "from scene.cameras import Camera, MiniCam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from pyquaternion import Quaternion\n",
    "from scene.dataset_readers import ProjectionType\n",
    "\n",
    "dataset = \"berlin\"\n",
    "eval_path = \"/home/amai/gaussian-splatting-merge/eval.znf\"\n",
    "# eval_path = str(Path(\"/cold/gaussian-splatting-merge.bk2/eval.znf\"))\n",
    "output_path = Path(f\"/home/amai/Videos/rotating_rooms/render\")\n",
    "\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "def load_path(path):\n",
    "    f = open(path, \"rb\")\n",
    "    data = f.read()\n",
    "    N = int.from_bytes(data[:4])\n",
    "    camera_size = 11\n",
    "\n",
    "    cameras = np.array(struct.unpack(f'>{N*camera_size}f', data[4:])).reshape(N, -1)\n",
    "    full_data = struct.unpack(f'>i{N*camera_size}f', data)\n",
    "    return cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00167627 -0.12787477 -0.99178891]\n"
     ]
    }
   ],
   "source": [
    "plane_points = load_path(\"/home/amai/Videos/rotating_rooms/cutting_plane.path\")[:, :3]\n",
    "point_on_plane = plane_points[0:1]\n",
    "normed = plane_points-point_on_plane\n",
    "evals, evecs = np.linalg.eig(normed.T @ normed)\n",
    "normal = np.cross(evecs[0], evecs[1])\n",
    "print(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = load_path(\"/home/amai/Videos/rotating_rooms/video_path.path\")#[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amai/gaussian-splatting-merge/eval.znf/berlin\n",
      "Looking for config file in /home/amai/gaussian-splatting-merge/eval.znf/berlin/cfg_args\n",
      "Config file found: /home/amai/gaussian-splatting-merge/eval.znf/berlin/cfg_args\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams\n",
    "\n",
    "def get_combined_args(args_cmdline):\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(args_cmdline.model_path, \"cfg_args\")\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file not found at\")\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k, v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "    return Namespace(**merged_dict)\n",
    "\n",
    "\n",
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "args = parser.parse_args(shlex.split(f\"-m {Path(eval_path) / dataset} --images images_4 -r 1\"))\n",
    "print(args.model_path)\n",
    "args = get_combined_args(args)\n",
    "model = model.extract(args)\n",
    "# model.source_path = str(Path(\"/data/nerf_synthetic\") / dataset)\n",
    "# model.source_path = str(Path(\"/data/nerf_datasets/tandt/\") / dataset)\n",
    "model.source_path = str(Path(\"/data/nerf_datasets/zipnerf\") / dataset)\n",
    "\n",
    "model.max_opacity = 0.99\n",
    "\n",
    "pipeline = pipeline.extract(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.max_opacity: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model at iteration 30000\n",
      "Reading camera 1525/1525\n",
      "Loading Training Cameras\n",
      "Loaded Train Cameras: 1525\n",
      "Loaded Test Cameras: 0\n"
     ]
    }
   ],
   "source": [
    "gaussians = GaussianModel(model.sh_degree, model.max_opacity)\n",
    "scene = Scene(model, gaussians, load_iteration=-1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "refcam = scene.getTrainCameras()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_renderer.fast_renderer import FastRenderer\n",
    "\n",
    "renderer = FastRenderer(refcam, gaussians, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_locations = gaussians._xyz.data.clone()\n",
    "original_rots = gaussians.get_rotation.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_points = load_path(\"/home/amai/Videos/rotating_rooms/ceiling_plane.path\")[:, :3]\n",
    "origin = axis_points[0:1]\n",
    "direction = axis_points[-1:] - origin\n",
    "direction = (direction / np.linalg.norm(direction)).reshape(1, -1)\n",
    "origin = torch.as_tensor(origin).float().cuda()\n",
    "direction = torch.as_tensor(direction).float().cuda()\n",
    "\n",
    "ceiling_mask = (direction @ (original_locations - origin).T > 0).reshape(-1)\n",
    "\n",
    "axis_points = load_path(\"/home/amai/Videos/rotating_rooms/floor_plane.path\")[:, :3]\n",
    "origin = axis_points[0:1]\n",
    "direction = axis_points[-1:] - origin\n",
    "direction = (direction / np.linalg.norm(direction)).reshape(1, -1)\n",
    "origin = torch.as_tensor(origin).float().cuda()\n",
    "direction = torch.as_tensor(direction).float().cuda()\n",
    "\n",
    "floor_mask = (direction @ (original_locations - origin).T > 0).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_points = load_path(\"/home/amai/Videos/rotating_rooms/rotation_axis.path\")[:, :3]\n",
    "origin = axis_points[0:1]\n",
    "direction = axis_points[-1:] - origin\n",
    "direction = (direction / np.linalg.norm(direction)).reshape(1, -1)\n",
    "origin = torch.as_tensor(origin).float().cuda()\n",
    "direction = torch.as_tensor(direction).float().cuda()\n",
    "\n",
    "mask = (direction @ (original_locations - origin).T > 0).reshape(-1)\n",
    "mask_offset = (direction @ (original_locations - origin + direction * 0.2).T > 0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_points = load_path(\"/home/amai/Videos/rotating_rooms/rotation_axis2.path\")[:, :3]\n",
    "cut_points = load_path(\"/home/amai/Videos/rotating_rooms/cutting_plane2.path\")[:, :3]\n",
    "cut_points3 = load_path(\"/home/amai/Videos/rotating_rooms/cutting_plane3.path\")[:, :3]\n",
    "\n",
    "origin1 = axis_points[0:1]\n",
    "direction1 = axis_points[-1:] - origin1\n",
    "direction1 = (direction1 / np.linalg.norm(direction1)).reshape(1, -1)\n",
    "origin1 = torch.as_tensor(origin1).float().cuda()\n",
    "direction1 = torch.as_tensor(direction1).float().cuda()\n",
    "\n",
    "origin2 = cut_points[0:1]\n",
    "direction2 = cut_points[-1:] - origin2\n",
    "direction2 = (direction2 / np.linalg.norm(direction2)).reshape(1, -1)\n",
    "origin2 = torch.as_tensor(origin2).float().cuda()\n",
    "direction2 = torch.as_tensor(direction2).float().cuda()\n",
    "\n",
    "origin3 = cut_points3[0:1]\n",
    "direction3 = cut_points3[-1:] - origin3\n",
    "direction3 = (direction3 / np.linalg.norm(direction3)).reshape(1, -1)\n",
    "origin3 = torch.as_tensor(origin3).float().cuda()\n",
    "direction3 = torch.as_tensor(direction3).float().cuda()\n",
    "\n",
    "half_plane1 = (direction1 @ (original_locations - origin1).T > 0).reshape(-1)\n",
    "half_plane2 = (direction2 @ (original_locations - origin2).T > 0).reshape(-1)\n",
    "half_plane3 = (direction3 @ (original_locations - origin3).T > 0).reshape(-1)\n",
    "\n",
    "mask2 = half_plane1 & half_plane2 & half_plane3\n",
    "\n",
    "mask = mask# & ceiling_mask & floor_mask\n",
    "mask2 = mask2 & ceiling_mask# & floor_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_quaternions(q, r):\n",
    "  \"\"\"\n",
    "  Rotates quaternions 'q' by another quaternion 'r'.\n",
    "\n",
    "  Args:\n",
    "      q: A tensor of quaternions to be rotated, shape (..., 4)\n",
    "      r: A tensor of quaternions representing the rotation, shape (..., 4)\n",
    "\n",
    "  Returns:\n",
    "      A tensor of rotated quaternions, shape (..., 4)\n",
    "  \"\"\"\n",
    "\n",
    "  # Ensure both q and r are normalized\n",
    "  q = q / torch.norm(q, dim=-1, keepdim=True)\n",
    "  r = r / torch.norm(r, dim=-1, keepdim=True)\n",
    "\n",
    "  # Extract quaternion components for easier manipulation\n",
    "  qw, qx, qy, qz = torch.unbind(q, dim=-1)\n",
    "  rw, rx, ry, rz = torch.unbind(r, dim=-1)\n",
    "\n",
    "  # Perform the quaternion multiplication (Hamilton product)\n",
    "  rotated_qw = rw * qw - rx * qx - ry * qy - rz * qz\n",
    "  rotated_qx = rw * qx + rx * qw + ry * qz - rz * qy\n",
    "  rotated_qy = rw * qy - rx * qz + ry * qw + rz * qx\n",
    "  rotated_qz = rw * qz + rx * qy - ry * qx + rz * qw\n",
    "\n",
    "  # Stack the rotated components back into a quaternion tensor\n",
    "  rotated_q = torch.stack([rotated_qw, rotated_qx, rotated_qy, rotated_qz], dim=-1)\n",
    "\n",
    "  return rotated_q\n",
    "\n",
    "def transform(points, rots, direction, origin, angle):\n",
    "    quat = Quaternion(axis=direction.cpu().numpy().reshape(-1), radians=angle)\n",
    "    quat_t = torch.tensor([quat.w, quat.x, quat.y, quat.z]).float().cuda().reshape(1, -1)\n",
    "    # quat_t = torch.tensor([1, 0, 0, 0]).float().cuda().reshape(1, -1)\n",
    "    new_rots = rotate_quaternions(rots, quat_t)\n",
    "    R = torch.as_tensor(quat.rotation_matrix).float().cuda()\n",
    "    rotated_pts = R @ (points - origin).T\n",
    "    return rotated_pts.T + origin, new_rots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [06:16<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "background = torch.tensor([0, 0, 0], dtype=torch.float32, device=\"cuda\")\n",
    "# width = 1280\n",
    "# height = 720\n",
    "# width = 1200\n",
    "# height = 667\n",
    "width = refcam.image_width\n",
    "height = refcam.image_height\n",
    "image = torch.ones((3, height, width), dtype=float)\n",
    "for i in tqdm(range(cameras.shape[0])):\n",
    "    start = 160\n",
    "    end = 260\n",
    "    start2 = 350\n",
    "    end2 = 580\n",
    "    angle = (i-start2) / (end2-start2) * 2 * np.pi if i > start2 and i < end2 else 0\n",
    "    # angle = 0\n",
    "    locs, rots = transform(original_locations[mask], original_rots[mask], direction, origin, angle)\n",
    "    angle2 = -(i-start)/(end-start)*2*np.pi if i > start and i < end else 0\n",
    "    locs2, rots2 = transform(original_locations[mask2], original_rots[mask2], direction1, origin1, angle2)\n",
    "    gaussians._xyz.data[mask] = locs\n",
    "    gaussians._rotation.data[mask] = rots\n",
    "    gaussians._xyz.data[mask2] = locs2\n",
    "    gaussians._rotation.data[mask2] = rots2\n",
    "    # print(locs, original_locations[mask])\n",
    "\n",
    "    T = cameras[i, :3]\n",
    "    # xyzw\n",
    "    quat = cameras[i, 3:7]\n",
    "    R = Quaternion(x=quat[0], y=quat[1], z=quat[2], w=quat[3]).transformation_matrix\n",
    "    R[:3, 3] = T\n",
    "    transf = np.linalg.inv(R).T\n",
    "    # print(R, transf)\n",
    "    # transf[1, :] = -transf[1, :]\n",
    "    # transf[2, :] = -transf[2, :]\n",
    "    transf[:, 1] = -transf[:, 1]\n",
    "    transf[:, 2] = -transf[:, 2]\n",
    "\n",
    "    # R = transf[:3, :3]\n",
    "    # T = transf[:3, 3]\n",
    "    fovy = cameras[i, -4]\n",
    "    fovx = cameras[i, -3]\n",
    "    fovy = refcam.FoVy\n",
    "    fovx = refcam.FoVx\n",
    "    znear = cameras[i, -2]\n",
    "    zfar = cameras[i, -1]\n",
    "    # view = Camera(0, R, T, aspect*fovy/180*np.pi, fovy/180*np.pi, image, image, \"fake\", 0)\n",
    "    world_view_transform = torch.as_tensor(transf).float()\n",
    "    full_proj_transform = torch.as_tensor(transf).float()\n",
    "    # fovx = 1.699109673500061\n",
    "    # fovx = 1.7087104320526123\n",
    "    # fovx = 1.399527668952942\n",
    "    view = MiniCam(width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform)\n",
    "    view.model = ProjectionType.PERSPECTIVE\n",
    "    with torch.no_grad():\n",
    "        rendering = splinerender(view, gaussians, pipeline, background, random=False)[\"render\"]\n",
    "        renderer.set_camera(view)\n",
    "        # rendering = renderer.render(view, pipeline, background)\n",
    "        # rendering = splinerender(cam, gaussians, pipeline, background)[\"render\"]\n",
    "        byte_rendering = (rendering.permute(1, 2, 0).cpu().numpy()*255).clip(min=0, max=255).astype(np.uint8)\n",
    "    full_output_path = output_path / f\"{i:06d}.png\"\n",
    "    imageio.imwrite(str(full_output_path), byte_rendering)\n",
    "    # plt.imshow(byte_rendering)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
